<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>CapaBench: Modular Attribution Benchmark</title>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap/dist/css/bootstrap.min.css">
  <style>
    body {
      font-family: Arial, sans-serif;
      line-height: 1.8;
      font-size: 1.2rem;
      margin: 0;
      padding: 0;
    }

    header {
      background: linear-gradient(90deg, #4b6cb7, #182848);
      color: white;
      padding: 40px 0;
    }

    header h1 {
      font-size: 3rem;
      margin-bottom: 10px;
    }

    header p.subtitle {
      font-size: 1.8rem;
      font-weight: 300;
    }

    .authors,
    .institutions {
      font-size: 1.1rem;
      margin-top: 10px;
      text-align: center;
      max-width: 900px;
      margin: 10px auto;
    }

    .container {
      max-width: 900px;
      margin: 0 auto;
      padding: 20px;
    }

    section {
      padding: 30px 0;
    }

    section h2 {
      font-size: 2.2rem;
      margin-bottom: 20px;
      border-bottom: 3px solid #4b6cb7;
      display: inline-block;
    }

    table {
      width: 100%;
      border-collapse: collapse;
      margin-top: 20px;
      font-size: 1rem;
    }

    table th,
    table td {
      border: 1px solid #ccc;
      padding: 10px;
      text-align: center;
    }

    table th {
      background-color: #f8f9fa;
      font-weight: bold;
    }

    .checkmark {
      color: purple;
      font-size: 1.5rem;
    }

    footer {
      background: #f8f9fa;
      padding: 20px 0;
      font-size: 1rem;
      text-align: center;
    }

    img {
      max-width: 100%;
      height: auto;
      margin: 20px 0;
    }
  </style>
</head>

<body>
  <!-- Header -->
  <header class="text-center">
    <h1>CapaBench</h1>
    <p class="subtitle">A Game-Theoretic Evaluation Benchmark for Modular Attribution in LLM Agents</p>
    <p><strong>Authors:</strong></p>
    <div class="authors">
      Yingxuan Yang<sup>1</sup>, Bo Huang<sup>1</sup>, Siyuan Qi<sup>1</sup>, Chao Feng<sup>1</sup>,
      Haoyi Hu<sup>1</sup>, Yuxuan Zhu<sup>2</sup>, Jinbo Hu<sup>1</sup>, Haoran Zhao<sup>1</sup>,
      Ziyi He<sup>3</sup>, Xiao Liu<sup>4</sup>, Zongyu Wang<sup>4</sup>, Lin Qiu<sup>4</sup>,
      Xuezhi Cao<sup>4</sup>, Xunliang Cai<sup>4</sup>, Yong Yu<sup>1</sup>, Weinan Zhang<sup>1</sup>
    </div>
    <p><strong>Institutions:</strong></p>
    <p class="institutions">
      <sup>1</sup>Shanghai Jiao Tong University,
      <sup>2</sup>University of Chicago,
      <sup>3</sup>University of Toronto,
      <sup>4</sup>Meituan
    </p>
    <p>Contact: <a href="mailto:zoeyyx@sjtu.edu.cn" class="text-light">zoeyyx@sjtu.edu.cn</a>,
      <a href="mailto:wnzhang@sjtu.edu.cn" class="text-light">wnzhang@sjtu.edu.cn</a>
    </p>
    <a href="CapaBench.pdf" class="btn btn-primary mx-2">Download Paper</a>
    <a href="https://github.com/your-repo" class="btn btn-secondary mx-2">View on GitHub</a>
  </header>

  <!-- Main Content -->
  <main class="container">
    <!-- Introduction -->
    <section id="intro" class="text-start">
      <h2>Introduction</h2>
      <p>
        CapaBench is a novel evaluation framework leveraging Shapley Value from cooperative game theory to measure
        the contributions of individual modules within modular Large Language Models (LLMs). By quantifying these
        contributions, CapaBench facilitates systematic optimization and enhances the interpretability of agent
        architectures.
      </p>
      <p>
        The framework evaluates Planning, Reasoning, Action, and Reflection capabilities by analyzing their
        individual and synergistic impacts on task performance. With a comprehensive dataset of over 1,500 multi-round,
        practical task scenarios, CapaBench enables robust and generalizable assessments tailored to real-world
        applications.
      </p>
    </section>

    <!-- Framework Overview -->
    <section id="framework" class="text-start">
      <h2>Framework Overview</h2>
      <h3>Agent Workflow</h3>
      <img src="images/Agent workfolw.png" alt="Agent Workflow" class="img-fluid">
      <p>
        The modular architecture of CapaBench consists of four key components:
      </p>
      <ul class="text-start mx-auto">
        <li><strong>Planning:</strong> Decomposes complex tasks into structured subtasks, enabling efficient resource
          allocation.</li>
        <li><strong>Reasoning:</strong> Uses logical inference and contextual understanding to determine appropriate
          actions.</li>
        <li><strong>Action:</strong> Translates cognitive processes into executable operations, ensuring effective task
          execution.</li>
        <li><strong>Reflection:</strong> Analyzes outcomes to iteratively improve performance through feedback and
          adjustments.</li>
      </ul>
      <p>
        This workflow allows agents to tackle diverse and complex scenarios, from e-commerce to collaborative robotics.
      </p>
    </section>

    <!-- Shapley Value Illustration -->
    <section id="shapley" class="my-5 text-start">
      <h2>Shapley Value Illustration</h2>
      <img src="images/shapley-value-illustration.png" alt="Shapley Value Illustration" class="img-fluid">
      <p>
        The Shapley Value, a cornerstone of cooperative game theory, provides a mathematically rigorous method for
        quantifying the marginal contributions of individual modules in an agent's architecture. This ensures fair
        attribution of credit based on all possible permutations of module combinations.
      </p>
      <p>
        For example, in a task requiring Planning, Reasoning, and Action, the Shapley Value captures the unique
        contribution of each module and their interactions. This enables a deeper understanding of how modules work
        together to drive performance.
      </p>
    </section>

    <!-- Dataset Construction -->
    <section id="dataset" class="text-start">
      <h2>Dataset Construction</h2>
      <table>
        <thead>
          <tr>
            <th></th>
            <th></th>
            <th colspan="2">Daily Activities</th>
            <th colspan="3">Computation</th>
            <th colspan="2">Role Control</th>
          </tr>
          <tr>
            <th></th>
            <th></th>
            <th>Shopping</th>
            <th>Navigation</th>
            <th>Math</th>
            <th>Theorem Proving</th>
            <th>OS</th>
            <th>Robot</th>
            <th>Game</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td rowspan="2">Planning</td>
            <td>Task Steps</td>
            <td><span class="checkmark">✔</span></td>
            <td></td>
            <td><span class="checkmark">✔</span></td>
            <td><span class="checkmark">✔</span></td>
            <td></td>
            <td></td>
            <td></td>
          </tr>
          <tr>
            <td>Resource Constraints</td>
            <td></td>
            <td><span class="checkmark">✔</span></td>
            <td></td>
            <td></td>
            <td><span class="checkmark">✔</span></td>
            <td><span class="checkmark">✔</span></td>
            <td><span class="checkmark">✔</span></td>
          </tr>
          <tr>
            <td rowspan="3">Reasoning</td>
            <td>Logical Validation</td>
            <td></td>
            <td></td>
            <td><span class="checkmark">✔</span></td>
            <td><span class="checkmark">✔</span></td>
            <td></td>
            <td></td>
            <td></td>
          </tr>
          <tr>
            <td>Knowledge Inference</td>
            <td><span class="checkmark">✔</span></td>
            <td><span class="checkmark">✔</span></td>
            <td></td>
            <td></td>
            <td></td>
            <td></td>
            <td><span class="checkmark">✔</span></td>
          </tr>
          <tr>
            <td>Text Understanding</td>
            <td></td>
            <td></td>
            <td></td>
            <td></td>
            <td></td>
            <td><span class="checkmark">✔</span></td>
            <td></td>
          </tr>
          <tr>
            <td>Action</td>
            <td>Environment Interaction</td>
            <td><span class="checkmark">✔</span></td>
            <td><span class="checkmark">✔</span></td>
            <td><span class="checkmark">✔</span></td>
            <td><span class="checkmark">✔</span></td>
            <td><span class="checkmark">✔</span></td>
            <td><span class="checkmark">✔</span></td>
            <td><span class="checkmark">✔</span></td>
          </tr>
          <tr>
            <td>Reflection</td>
            <td>Failure Analysis</td>
            <td><span class="checkmark">✔</span></td>
            <td><span class="checkmark">✔</span></td>
            <td><span class="checkmark">✔</span></td>
            <td><span class="checkmark">✔</span></td>
            <td><span class="checkmark">✔</span></td>
            <td><span class="checkmark">✔</span></td>
            <td><span class="checkmark">✔</span></td>
          </tr>
        </tbody>
      </table>
    </section>

    <!-- Experiment Results -->
    <section id="results" class="text-start my-5">
      <h2>Experiment Results</h2>
      <h3>Selected Results Across Datasets</h3>
      <img src="images/main_results.jpg" alt="Experimental Results Across Datasets" class="img-fluid">
      <p class="mt-4">
        The image above highlights results for selected models and datasets. Blue cells represent baseline model
        metrics,
        and bold entries indicate the best results achieved for each task. Additional analysis confirms that module
        configurations
        with higher Shapley Value contributions consistently outperform others.
      </p>

      <h3>Key Findings</h3>

      <p>
        Based on the experimental results, we derived the following key insights:
      </p>

      <h4>Cross-Task Model Performance Comparison</h4>
      <p>
        Models such as <strong>Claude-3.5</strong> excel in complex logical reasoning and collaborative tasks, including
        formal verification
        (e.g., Coq, Lean 4, Isabelle) and robot cooperation. In contrast, open-source models like
        <strong>Qwen-2.5</strong>
        and <strong>Mistral-8X7B</strong> demonstrate strong performance in simpler tasks like shopping and algebra but
        underperform
        in cognitively demanding scenarios. These results highlight that while open-source models succeed in specific
        domains,
        proprietary models often dominate in tasks requiring advanced reasoning and collaboration.
      </p>

      <h4>Module Contribution Patterns</h4>
      <p>
        Module contributions vary significantly by task, as summarized below:
      </p>
      <ul class="text-start mx-auto" style="max-width: 800px;">
        <li><strong>Online Shopping Tasks:</strong> The Planning ($P_t$) and Reasoning ($R_t$) modules dominate,
          emphasizing strategic decision-making and logical reasoning.</li>
        <li><strong>Math Solver Tasks (Algebra and Geometry):</strong> The Action ($A_t$) module is critical, especially
          in geometry, where precise execution is key.</li>
        <li><strong>Formal Verification Tasks:</strong> The Action ($A_t$) module is dominant, particularly for formal
          language tasks like Coq, requiring precision.</li>
        <li><strong>Robot Cooperation Tasks:</strong> The Reasoning ($R_t$) module plays the largest role, supporting
          coordinated multi-agent actions.</li>
        <li><strong>Operating System Tasks:</strong> The Reasoning ($R_t$) module is crucial for dynamic problem-solving
          within OS environments.</li>
      </ul>

      <h4>Ability Transfer Patterns</h4>
      <p>
        Foundational skills like planning, reasoning, and execution generalize across related tasks. For instance:
        <br>
        - Proficiency in the Action ($A_t$) module for Formal Verification translates into strong performance in math
        problem-solving tasks like algebra and geometry.<br>
        - Planning ($P_t$) abilities in Shopping tasks transfer to Robot Cooperation due to shared requirements for
        multi-step coordination and optimization.
      </p>

      <h4>Impact of Model Architecture and Training Strategies</h4>
      <p>
        Model architecture and training strategies often outweigh raw parameter size in determining performance. For
        example:
        <br>
        - <strong>Qwen-2.5</strong> outperforms in algebra and geometry tasks despite fewer parameters, thanks to its
        optimized Action ($A_t$) module.<br>
        - <strong>Mistral-8X7B</strong>, though larger in size, underperforms in several tasks, underscoring the
        importance of innovative design and training over sheer scale.
      </p>

    </section>

    <!-- Citation -->
    <section id="citation" class="text-start my-5">
      <h2>Citation</h2>
      <pre>
@article{capabench2025,
  title={CapaBench: A Game-Theoretic Evaluation Benchmark for Modular Attribution in LLM Agents},
  author={Yingxuan Yang and others},
  journal={Conference Name},
  year={2025},
}
  </pre>
    </section>


    <!-- Footer -->
    <footer>
      <p>© 2025 CapaBench Team. All Rights Reserved.</p>
    </footer>
</body>

</html>